---
format: 
  revealjs:
    controls: true
    slide-number: c
    logo: images/PH_logo.svg
    theme: [dark, css/custom.scss]
    preview-links: auto
    chalkboard:
      theme: whiteboard
      boardmarker-width: 3
      buttons: true
      chalk-effect: 0
    fullscreen: true
    pdfexport: true
    multiplex: true
    auto-stretch: false
bibliography: references.bib
csl: apa.csl
editor_options: 
  chunk_output_type: console
---

## <font color="#8cd000">Teachers trust scientific evidence...</font>{.cent2}


```{r hidden chunk which creates template stuff}
#| echo: false

########################
library(fontawesome)
library(tidyverse)
library(readxl)
library(ggalt)
library(hrbrthemes)
library(reactable)
set.seed(848265)

# Change css to lecker PH green
#if(!dir.exists("img"))
#dir.create("img")
if(!dir.exists("css"))
dir.create("css")
fileConn<-file("css/custom.scss")
fileStyle<-file("css/custom.scss")
writeLines(c("/*-- scss:defaults --*/",
             "$link-color: #8cd000 !default;",
             ".imp {color: #8cd000;}",
             ".imp2 {color: #50B32E;}",
             ".cent2 h2 {text-align: center;}",
             ".cent3 h3 {text-align: center;}",
             ".cent1 h1 {text-align: center;}",
             ".cent_text {text-align: center;}"), fileConn)
close(fileConn)
close(fileStyle)
```

```{css}

.em09 {
font-size: .9em;}

.em08 {font-size: .8em;}

.em07 {font-size: .7em;}

```


<center>
<font color="#8cd000">...especially if it confirms their beliefs</font>
</center>


<br>

::: {style="font-size: 0.6em"} 
<center> Kirstin Schmidt</u><sup>1</sup>, Tom Rosman<sup>2</sup>, Colin Cramer<sup>3</sup>, <br> Kris-Stephen Besa<sup>4</sup> and Samuel Merk<sup>1</sup> </center>

<br>
<center> 
<sup>1</sup> Karlsruhe University of Education <br>
<sup>2</sup> ZPID - Leibniz-Institut for Psychology Trier <br>
<sup>3</sup> Eberhard Karls University Tübingen <br>
<sup>4</sup> University Münster <br>
<br>
<br>

presentation available at [https://kirstinschmidt.github.io/EARLI_presentation/]{preview-link="true"} <br>
paper available at [https://doi.org/10.3389/feduc.2022.976556]{preview-link="true"} <br>
[Paper presentation EARLI 2023 | 26.08.2023]{.imp2}
</center>

:::

::: notes
- hear me in the back? 
- happy to present an experimental study in which my colleagues and I have analyzed how teachers trust in scientific evidence

:::

## <font color="#8cd000">[Structure]{style="font-size: 0.8em"} {{< fa list-ul >}}</font>

:::{style="font-size: 0.7em"}
* Evidence-informed school practice {{< fa school >}}
* Trust in scientific evidence {{< fa hands-helping >}}
  * Trust in different information sources {{< fa list >}}
  * Trust and confirmation bias {{< fa eye-slash >}}
* Experimental study - Teachers' beliefs and their influence on trust {{< fa search >}}
  * Hypotheses {{< fa exclamation >}}
  * Sample {{< fa users >}}
  * Design and materials {{< fa align-left >}}
  * Results {{< fa chart-bar >}}
* Discussion {{< fa comments >}}
* References {{< fa book-open >}}
:::

::: notes

- In classical manner, I will start with a quick introduction to the theoretical framework of the study 
- afterwards present our study design and results
- before discussing the results

:::

# <font color="#8cd000">Evidence-informed school practice {{< fa school >}}</font>{.cent1}

::: notes

- let's start with the theoretical framework: the concept evidence-informed school practice

:::

## <font color="#8cd000">[Evidence-informed school practice]{style="font-size: 0.8em"} {{< fa school >}}</font>

:::{style="font-size: 0.8em"}
* Teachers can draw on different information sources when making professional decisions; information sources range from anecdotal evidence to scientific evidence (e.g., Buehl & Fives, 2009)

* [Evidence-informed school practice means to consider, among others, scientific evidence in professional actions e.g., to develop new practices]{.imp2}
<br>
  &rarr; Increasing school quality, teaching quality and students' achievement <br> (e.g., Brown et al., 2017)
<br>

* However, teachers often do not consider scientific evidence in their school practice (e.g., Brown et al., 2017; Hinzke et al., 2020)

:::

::: notes

- teachers can draw on different information sources when it comes to professional actions e.g., anecdotal evidence such as advises from colleagues or scientific evidence - findings from a research study
- considering scientific evidence in school practice - so called evidence-informed practice - is increasingly valued by policy makers and scientists because scientific evidence is regarded as best available knowledge and a more objective information source
- in school practice it can be used to e.g., develop new practices or solve problems that have repeatedly emerge (to increase school quality)
- However, despite this widespread demand, teachers often do not act in an evidence-informed manner
:::


## <font color="#8cd000">[Evidence-informed school practice]{style="font-size: 0.8em"} {{< fa school >}}</font>

:::{style="font-size: 0.8em"}
Numerous barriers exist that make it difficult for teachers to act in an evidence-informed manner:

::: {.incremental}
  * Resources (e.g., limited time)
  * Belief systems (e.g., negative beliefs about the practical applicability of scientific knowledge)
  * Cognitive abilities / competencies (e.g., lack of statistical knowledge) <br>
  (e.g., Brown et al., 2022; van Schaik et al., 2018)
:::

:::

::: notes
- previous research shows that numerous barriers exist, I am just naming a few of the identified barriers to get an impression what hinders teachers from engaging in an evidence-informed manner
  - resources 
  - belief systems 
  - cognitive abilities/competencies 
:::
  

## <font color="#8cd000">[Evidence-informed school practice]{style="font-size: 0.8em"} {{< fa school >}}</font>

:::{style="font-size: 0.8em"}
Numerous barriers exist that make it difficult for teachers to act in an evidence-informed manner:

* Resources (e.g., limited time)
* Belief systems (e.g., negative beliefs about the practical applicability of scientific knowledge)
* [__Cognitive abilities / competencies (e.g., lack of statistical knowledge)__]{.imp2}<br>
  (e.g., Brown et al., 2022; van Schaik et al., 2018)
:::

::: notes

- Sticking to the abilities/competencies
- teachers often struggle with so called first-hand evaluations that is interpreting and understanding scientific information as well as evaluating its veracity 
- but there is also an other way to evaluate the information 

:::
  

# <font color="#8cd000">Trust in scientific evidence {{< fa hands-helping >}}</font> {.cent1}

::: notes

- that is evaluating its trustworthiness
:::

## <font color="#8cd000">[Trust in scientific evidence]{style="font-size: 0.8em"} {{< fa hands-helping >}}</font>

:::{style="font-size: 0.8em"}
* [Teachers often evaluate scientific evidence second-hand: instead of evaluating its veracity, they evaluate its credibility and trustworthiness]{.imp2} <br>
(e.g., Merk & Rosman, 2019)
<br> &rarr; Trust = key predictor/indicator to what extent scientific evidence is positively evaluated and considered (Bromme et al.,2022; Hendriks et al., 2016)
<br>

* Subjective criteria such as teachers' beliefs can influence the evaluation of trust <br> (e.g., Dietz & Den Hartong, 2006; Blöbaum, 2016)

:::

::: notes

- if teachers evaluate the extent to which an information, e.g., scientific evidence, appears trustworthy, they conduct what is known as second-hand evaluation
  - hence, teachers do not ask to what extent the statement is true - that would be first-hand evaluation - but to what extent they can trust the statement 

- teachers often choose this second-hand evaluation
- hence, trust can be a central predictor/indicator for the extent to which teachers actually consider scientific evidence in their school practice


::: 


## <font color="#8cd000">[Trust in scientific evidence]{style="font-size: 0.8em"} {{< fa hands-helping >}}</font>

:::{style="font-size: 0.8em"}
* [Teachers often evaluate scientific evidence second-hand: instead of evaluating its veracity, they evaluate its credibility and trustworthiness]{.imp2} <br>
(e.g., Merk & Rosman, 2019)
<br> &rarr; Trust = key predictor/indicator to what extent scientific evidence is positively evaluated and considered (Bromme et al.,2022; Hendriks et al., 2016)
<br>

* [__Subjective criteria such as teachers' beliefs can influence the evaluation of trust__]{.imp2} <br> (e.g., Dietz & Den Hartong, 2006; Blöbaum, 2016)

:::

::: notes

- however, this evaluation of trustworthiness can be influenced by different subjective criteria - such as teachers' belief systems which might also act as barriers for evidence-informed practice / not free of distortions 

:::

## <font color="#8cd000">[Trust in different information sources]{style="font-size: 0.8em"} {{< fa list >}}</font>

:::{style="font-size: 0.8em"}
[(Future) teachers primarily consider anecdotal evidence in their practice (e.g., Buehl & Fives, 2009) and have more trust in other teachers than in scientists:]{.imp2}

* Especially when looking for practical advice (Hendriks et al., 2021)
* Educational scientists = "smart but evil" (Merk & Rosman, 2019)
* Lack of integrity and benevolence as reasons for distrust in educational scientists (Rosman & Merk, 2021)

:::

::: notes

- beliefs about sources of evidence itself can influence trust ratings
- findings from previous studies show that teachers primarily consider anecdotal evidence  in their practice 
- and also trust information from teachers more than information from scientists especially when it comes to practical advises

:::

## <font color="#8cd000">[Trust in different information sources]{style="font-size: 0.8em"} {{< fa list >}}</font>

:::{style="font-size: 0.8em"}
[(Future) teachers primarily consider anecdotal evidence in their practice (e.g., Buehl & Fives, 2009) and have more trust in other teachers than in scientists:]{.imp2}

* Especially when looking for practical advice (Hendriks et al., 2021)
* Educational scientists = "smart but evil" (Merk & Rosman, 2019)
* Lack of integrity and benevolence as reasons for distrust in educational scientists (Rosman & Merk, 2021)


[{{< fa exclamation-triangle >}} __Anecdotal evidence that is not supported by science (e.g., the learning style theory) is highly prevalent in everyday school life and may lead to ineffective or dysfunctional practice__]{.imp}

:::

::: notes

- the basically higher trust in anecdotal evidence can be problematic if anecdotal evidence that is not supported by science is disseminated in everyday school life
- this (can) lead to ineffective or dysfunctional practice - think, for example, of common misconceptions such as learning styles
- not supported by evidence but very common among teachers

:::

## <font color="#8cd000">[Trust and confirmation bias]{style="font-size: 0.8em"} {{< fa eye-slash >}}</font>

:::{style="font-size: 0.8em"}
[Confirmation bias = own prior beliefs influence searching for and interpreting information]{.imp2}

* Information that is consistent with prior beliefs is preferred or perceived as more confirmatory
* Information that is inconsistent with prior beliefs is more quickly devalued and thus avoided or analyzed thoroughly to find errors in it <br> (e.g., Hart et al., 2009; Nickerson, 1998; Stroud, 2017)
<br>
<br>

:::

::: notes

- in addition to beliefs about the trustworthiness of different sources, teachers prior beliefs about specific educational topics might also influence teachers' trust ratings
- one way of this influence can be: confirmation bias
- key note
- confirmation bias means that one's own prior beliefs influence the search for and interpretation of information
  - in the sense that 
    - information that is in line with one's own prior beliefs is preferred
    - whereas information that contradicts one's own beliefs is devalued
  
:::

## <font color="#8cd000">[Trust and confirmation bias]{style="font-size: 0.8em"} {{< fa eye-slash >}}</font>

:::{style="font-size: 0.8em"}
[In the educational context, confirmation bias is hardly systematically researched:]{.imp2}

* Teachers often do not look for data that challenge/question their own opinions (Van Lommel et al., 2017)
* When data is inconsistent with teachers' beliefs, teachers look for explanations as to why the data might be wrong (Andersen, 2020)
* Student teachers are more likely to display confirmation bias than hindsight bias when evaluating scientific studies (Masnick & Zimmermann, 2009)

:::

::: notes

- confirmation bias is often discussed as a barrier of evidence-informed practice, but it has not yet been systematically analyzed
- there are initial insights on the existence of confirmation bias in data-based decision making investigated using qualitative analyses
- but to the best of our knowledge only one quantitative study exists that focus on interpreting scientific evidence
- the study shows that teachers rather show a confirmation bias than a hindsight bias when evaluating scientific studies first-hand (e.g., evaluating the appropriateness of a study’s design)
  - thus, we wanted to investigate whether confirmation bias is also apparent in second-hand evaluation

::: 

## <font color="#8cd000">[Trust and confirmation bias]{style="font-size: 0.8em"} {{< fa eye-slash >}}</font>

:::{style="font-size: 0.8em"}
[In the educational context, confirmation bias is hardly systematically researched:]{.imp2}

* Teachers often do not look for data that challenge/question their own opinions (Van Lommel et al., 2017)
* When data is inconsistent with teachers' beliefs, teachers look for explanations as to why the data might be wrong (Andersen, 2020)
* Student teachers are more likely to display confirmation bias than hindsight bias when evaluating scientific studies (Masnick & Zimmermann, 2009)

[{{< fa exclamation-triangle >}} __A one-sided reference or lack of reference to scientific evidence can make it difficult or even impossible to adequately realize evidence-informed practice__]{.imp}

:::


::: notes

- because if this is the case, this could be a further barrier for evidence-informed practice
- confirmation bias might be problematic because it can lead to a one-sided or lack of reference to evidence

- to sum up: sources itself and confirmation bias might influence teachers' trust ratings

ADD AN EXAMPLE

further information:
This would make it difficult or even impossible to adequately realize evidence-informed practice

::: 

# <font color="#8cd000">Experimental study - teachers' beliefs and their influence on trust {{< fa search >}} </font> {.cent1 .em09}

::: notes

- in our experimental study we have focused on both trust in specific sources and confirmation bias 

:::

## <font color="#8cd000">[Hypotheses]{style="font-size: 0.8em"} {{< fa exclamation >}} </font>

<br>

H<sub>1</sub>: When teachers are confronted with knowledge claims regarding specific topics from educational science, they show more trust in claims if these are allegedly from another teacher (anecdotal evidence) than from a scientific study (scientific evidence).

<br>

H<sub>2</sub>: When teachers are confronted with evidence for specific knowledge claims of educational science, they show more trust in these claims if these are belief-consistent.


::: notes

- examined:

1) whether teachers show more trust in certain claims from educational science when they come from a teacher (that is anecdotal evidence) than from a scientist (that is scientific evidence)
2) whether teachers trust certain claims from educational science more when they fit their prior beliefs - rather than refute them

further information:

- we also preregistered these hypotheses
:::

## <font color="#8cd000">[Sample]{style="font-size: 0.8em"} {{< fa users >}} </font>

[*N* = 414 teachers in Germany]{.imp}

* 73.1% female teachers
* 80.8% of the teachers are between 30 and 59 years old
* Teachers teach at different types of schools (z.B. 29.3% teach at a primary school; 25.8% teach at a upper secondary school)
<br>
<br>
[&rarr; »Representative« sample]{.imp} <br>(cf. Destatis, 2019)


::: notes

- we collected data from a representative sample of 414 teachers in Germany
- here you can see some information about the demographics of our sample

:::

## 


![`r shiny::HTML("<center>(cf. Schmidt et al., 2022)</center>")`](images/epitomes.png){fig-align="center" width=60% .fragment} 


<!--Das ist ein arg hässlicher Hack mit `shiny::HTML()` - aber mir fällt gerade nicht besseres ein -->




::: notes

- to test our hypotheses we presented teachers with in total two out of four different claims from educational science
  - these two claims were chosen randomly
  - the order whether the claim was made by a teacher or a scientist was randomized, as well and 
  - whether the claim was consistent or inconsistent to participants prior topic-specific beliefs was also randomized
- for both claims the procedure was always the same (click)
  - first we presented the teachers with the topic, e.g., integration of text and figure
  - then we asked the participants for their opinion on this topic
  - afterwards we showed them the respective claim - in this box - 
  - this claim was either consistent or inconsistent to the answer given before and either from a teacher or a scientist
  - finally, the participants were ask how much they trust the claim above 
- the second round was structured identically, but the source and consistency was the opposite to the first round
- now I am coming to the results

:::

---

```{r }
#| message: false
#| results: hide
#| cache: true
library(dplyr)
library(ggplot2)
library(tidyr)
library(readr)
library(mice)
library(naniar)
library(miceadds)
library(brms)
library(hrbrthemes)
library(survey)
library(patchwork)

data_long <- 
  read_csv("data/data_long.csv") %>% 
  mutate(across(c(Topic, Source, Consistency, lfdn), as.factor)) %>% 
  mutate(across(c(Trust, TrustInScience, TrustInEdScience), as.ordered))

weighted_means_sd <- 
  data_long %>% 
  mutate(Trust = as.numeric(Trust),
                  Source = recode_factor(Source,
                                `Scientific Study` = 
                                  "Scientific Evidence",
                                `Episodic Evidence` = 
                                  "Anecdotal Evidence"),
                  Topic = recode_factor(Topic,
                                        `Grades` =  
                                         "Grades",
                                        `Integration` =  
                                         "Integration",                                                                             `Retention` =  
                                         "Retention",
                                        `Signalling` =  
                                         "Signaling")
         ) %>%  
  group_by(Consistency, Source, Topic) %>% 
  summarize(w_mean = weighted.mean(Trust, w = weight0, na.rm = T),
            w_sd = sqrt(Hmisc::wtd.var(Trust, weights = weight0, na.rm = T)))


MeansSD_weighted_plot <- 
  ggplot(weighted_means_sd, aes(w_mean, Consistency)) +
  geom_vline(xintercept = 3, color = "darkgrey", linetype = "dashed") +
  facet_grid(Topic ~ Source) + 
  geom_point(size = 3, shape = 18) + 
  geom_linerange(aes(xmax = w_mean + w_sd, xmin = w_mean - w_sd),
                 width = .1) +
  theme_ipsum() + 
  xlim(c(1,5)) +
  ylab("") + 
  xlab("Trust") + 
  labs(title = "Trust", subtitle = "by Belief-Consistency, Source and Topic",
       caption="H1: d = -0.81 (grades: d = -1.25; integration: d = -1.04; retention: d = -0.26; signaling: d = -0.86); 
       H2: d = 0.40 (grades: d = 0.43; integration: d = 0.28; retention: d = 0.64; signaling: d = 0.25)")  

ggsave("trust_by_belief-consistency_source_topic.jpg", 
       MeansSD_weighted_plot,
       width = 6.1,
       height = 6.6 )
```


![`r shiny::HTML("<center>cf. Schmidt et al. (2022; p. 8)</center>")`](./trust_by_belief-consistency_source_topic.jpg){width=50%}


::: notes

- in this figure you can see the trust ratings differentiated per consistency on the left y-axis, per topic on the right y-axis and source of evidence: left column scientific evidence, right column anecdotal evidence
- what is quite obvious when we look at both source columns is that descriptively the average trust ratings for scientific evidence are higher than for anecdotal evidence
  - across all topics - which is quite surprising
- we can also see that teachers trust claims more if they fit their beliefs, i.e., when they are consistent with their beliefs 
  - descriptive result is in line with our hypothesis


further information:

- descriptives based on weighted data

::: 

## {.center}

![`r shiny::HTML("<center>cf. Schmidt et al. (2022; p. 9)</center>")`](images/table_results.png){width=100%}

::: notes 

now we are looking at the inferential statistics 
- we calculated different Bayesian multilevel models

- today only Model 2 and Model 5 are important
  - Model 2 focuses on the different sources (scientific vs. anecdotal evidence)
  - Model 5 focuses on the influence of Confirmation Bias (pinch to zoom)
  - Model 5a?
- I do not want to explain the results in detail, but what we can see is that our descriptive impression is also supported inferentially

further information:

- highest density intervals: HDIS included 0 and are rather small around zero -> evidence for null-hypothesis
- Bayes factors based on bridge sampling
- which we can see by looking at R2 (square) and the highest density intervals; slopes!

::: 

# <font color="#8cd000">[Discussion]{style="font-size: 0.8em"} {{< fa comments >}}</font>{.cent1}

::: notes

- how can we explain and discuss these results?
::: 

## <font color="#8cd000">[Discussion]{style="font-size: 0.8em"} {{< fa comments >}}</font>
:::{style="font-size: 0.7em"}
[{{< fa surprise >}}: In contrast to previous studies, teachers at hand showed greater trust in scientific evidence than in anecdotal evidence]{.imp2} <br>

* Trust in specific statements vs. trust in sources without content statements
* Influence of publication body (scientific journal vs. teacher blog) <br> &rarr; Information from social media is rated less trustworthy compared to scientific and anecdotal evidence (Braten & Ferguson, 2015)
<br>

:::

::: notes

- first, take a closer look at the surprising result that teachers in our study showed greater trust in scientific than in anecdotal evidence
- different possible explanations why our results and previous results differ. But I want to highlight two explanations that refer to the operationalization of the dependent or independent variable (source)

1) we have analyzed trust in specific statements to increase internal validity, but previous studies have analyzed trust in sources without using a content statement, i.e. only asking how much they trust statements from, for example, scientists
    - this may have led to a bias in that different sources may be associated with different topics/expertise. For example, teachers as teaching experts and scientists as experts in conducting scientific studies, i.e., practical vs. more abstract topics.
    - maybe not the case that teachers doubt that scientists can deliver trustworthy knowledge, but maybe they do not want to use scientific knowledge because it is often associated with abstract and theoretical information
  
  3) study vignettes included publication outlets - the outlet for anecdotal evidence was 'teacher blog' and for scientific evidence 'scientific journal'
  - chosen publication outlets might have confounded the results
  - a study by Braten & Ferguson showed that social media is rated less trustworthy. Hence, the publication outlet 'teacher blog' may have lead to lower trust ratings of anecdotal evidence 
  - we have chosen a written format for both sources to keep them as parallel as possible, but also as realistic as possible e.g., teachers often do not meet scientists in person


further information: 

But in studies with the multidimensional construct, lower trust in scientists, especially in terms of integrity and benevolence, but globally still rather high

(0) In the mentioned studies on teachers' trust in different sources, trust is often assessed as a multidimensional construct: expertise, integrity and benevolence. However, we have used the overall assessment of the German Science Barometer)


:::

## <font color="#8cd000">[Discussion]{style="font-size: 0.8em"} {{< fa comments >}}</font>
:::{style="font-size: 0.7em"}
[{{< fa surprise >}}: In contrast to previous studies, teachers at hand showed greater trust in scientific evidence than in anecdotal evidence]{.imp2} <br>

* Trust in specific statements vs. trust in sources without content statements
* Influence of publication body (scientific journal vs. teacher blog) <br> &rarr; Information from social media is rated less trustworthy compared to scientific and anecdotal evidence (Braten & Ferguson, 2015)
<br>


[{{< fa check-double >}}: Consistent with previous studies, teachers also show strong confirmation bias in second-hand evaluations]{.imp2} 
<br> &rarr; Selective trust could lead to one-sided use of scientific evidence
<br> &rarr; Confirmation bias as a filter to evalute scientific evidence
<br>

:::


::: notes

- our results on confirmation bias are consistent with previous research on first-hand evaluation 
- the results show that confirmation bias is also apparent in second-hand evaluations

- Taking both results together on source and confirmation bias, one could argue that it is not a general lack of trust in science that prevents teachers from considering scientific evidence in their school practice, rather which filters come into play to evaluate the evidence. Confirmation bias might work as such a filter
  - symposium: epistemic perspectives another filter
  - if teachers trust evidence that is consistent with their beliefs and rather distrust evidence that is contradictory, they will continue to teach as before
    - unproblematic as long as the teachers’ practice is tried and tested
    - but problematic when it for example comes to concepts that are not supported by evidence but continue to persist in practice
    - (or when one-sided evidence is used to develop new approaches or to overcome hitherto unsolved problems as this makes it difficult to stimulate new avenues)
    

further information: 

or when one-sided evidence is used to develop new approaches or to overcome hitherto unsolved problems as this makes it difficult to stimulate new avenues.

:::

## <font color="#8cd000">[Discussion]{style="font-size: 0.8em"} {{< fa comments >}}</font>
:::{style="font-size: 0.7em"}
[{{< fa surprise >}}: In contrast to previous studies, teachers at hand showed greater trust in scientific evidence than in anecdotal evidence]{.imp2} <br>

* Trust in specific statements vs. trust in sources without content statements
* Influence of publication body (scientific journal vs. teacher blog) <br> &rarr; Information from social media is rated less trustworthy compared to scientific and anecdotal evidence (Braten & Ferguson, 2015)
<br>


[{{< fa check-double >}}: Consistent with previous studies, teachers also show strong confirmation bias in second-hand evaluations]{.imp2} 
<br> &rarr; Selective trust could lead to one-sided use of scientific evidence
<br> &rarr; Confirmation bias as a filter to evalute scientific evidence
<br>
<br>

[{{< fa bolt >}}: In the present study trust, not engagement with or use of evidence, was measured]{.imp2}
<br> &rarr; Trust does not automatically imply evidence-informed actions
:::

::: notes


- in this regard, I finally want to highlight that we have only analyzed trust as a key predictor for the engagement with evidence
  - we did not directly measure engagement with or use of evidence
  - meaning trust does not automatically imply evidence-informed actions

- that would be an important direction for future studies to look at
- looking forward to question, comments, discussion 
  
  
further information

- familiarity of the source might be important aspect: trusted colleague more trustworthy than an anonymous blogposter; but in previous studies rather generic terms were used such as "experienced teacher" or "a teacher who has taught at a school for a number of years" 

- context in which the evidence is intended to be used (e.g., teachers appeared more trustworthy when it came to practical recommendations, Hendriks et al., 2021)

- name of the persons who provided the information (Quantz and Peters, 2014 vs. Mr. Mueller) or name of the publisher (Journal of Effective Teaching vs. Heartandsoulteacher.org)

- challenge of designing internally valid study materials: 
  - more information, the less abstract the information appears and the more varying associations of the participants can be prevented 
  - cannot be ruled out that one of these pieces of contextual information confounds the variables that are actually relevant for the study and introduces bias on the outcomes 

- effects of heterogeneity:
  - individual characteristics of teachers:
    - trust of educational science in general
    - degree of educational research literacy
    - epistemic beliefs (multiplistic belief - scientific knowledge as subjective opinions = science less trustworthy)
  - characteristics of scientific evidence:
    - research paradigms (experimental vs. observational research)
    - proximity of scientific evidence to teaching practice
  
:::

## <font color="#8cd000">[References]{style="font-size: 0.8em"} {{< fa book-open >}} </font>{.scrollable}

:::{style="font-size: 0.6em"}
Andersen, I. G. (2020). What went wrong? Examining teachers’ data use and instructional decision making through a bottom-up data intervention in Denmark. *International Journal of Educational Research, 102*, 101585. https://doi.org/10.1016/j.ijer.2020.101585 <br>
Blöbaum, B. (2016). Key factors in the process of trust. On the analysis of trust under digital conditions. In B. Blöbaum (Ed.), *In Trust and Communication in a Digitized World. Springer International Publishing* (pp. 3–25). https://doi.org/10.1007/978-3-319-28059-2_1 <br>
Braten, I., & Ferguson, L. E. (2015). Beliefs about sources of knowledge predict motivation for learning in teacher education. *Teaching and Teacher Education, 50*, 13–23. https://doi.org/10.1016/j.tate.2015.04.003 <br> 
Bromme, R., Kienhues, D., & Porsch, T. (2010). Who knows what and who can we believe? Epistemological beliefs are beliefs about knowledge (mostly) to be attained from others. In L. D. Bendixen & F. C. Feucht (Eds.), *Personal Epistemology in the Classroom*. Cambridge: Cambridge University Press (pp. 163–194). https://doi.org/10.1017/CBO9780511691904.006 <br>
Bromme, R., Mede, N. G., Thomm, E., Kremer, B., & Ziegler, R. (2022). An anchor in troubled times: Trust in science before and within the COVID-19 pandemic. *PLOS ONE, 17*, e0262823. https://doi.org/10.1371/journal.pone.0262823 <br>
Brown, C., Macgregor, S., Flood, J., & Malin, J. (2022). Facilitating Research-Informed Educational Practice for Inclusion. Survey Findings From 147 Teachers and School Leaders in England. *Frontiers in Education, 7*, 890832. https://doi.org/10.3389/feduc.2022.890832 <br>
Brown, C., Schildkamp, K., & Hubers, M. D. (2017). Combining the best of two worlds: A conceptual proposal for evidence-informed school improvement. *Educational Research, 59*(2), 154–172. https://doi.org/10.1080/00131881.2017.1304327 <br>
Buehl, M. M., & Fives, H. (2009). Exploring teachers’ beliefs about teaching knowledge: Where does it come from? Does it change? *Journal of Experimental Education, 77*, 367–407. https://doi.org/10.3200/JEXE.77.4.367-408 <br>
Destatis (2019). Bildung, Forschung und Kultur. Genesis-Online <br>
Dietz, G. & Den Hartog, D. N. (2006). Measuring trust inside organisations. *Personnel Review, 35*, 557–588. https://doi.org/10.1108/00483480610682299 <br>
Hart, W., Albarracin, D., Eagly, A. H., Brechan, I., Lindberg, M. J., & Merrill, L. (2009). Feeling validated versus being correct: A meta-analysis of selective exposure to information. *Psychological Bulletin, 135*, 555–588. https://doi.org/10.1037/a0015701 <br>
Hendriks, F., Kienhues, D., & Bromme, R. (2016). Trust in science and the science of trust. In B. Blöbaum (Ed.), *Trust and Communication in a Digitized World* (pp.143–159). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-28059-2_8 <br>
Hendriks, F., Seifried, E., & Menz, C. (2021). Unraveling the “smart but evil” stereotype: Pre-service teachers’ evaluations of educational psychology researchers versus teachers as sources of information. *Zeitschrift für Pädagogische Psychologie, 35*, 157–171. https://doi.org/10.1024/1010-0652/a000300 <br>
Hinzke, J.-H., Gesang, J., & Besa, K.-S. (2020). Zur Erschließung der Nutzung von Forschungsergebnissen durch Lehrpersonen. Forschungsrelevanz zwischen Theorie und Praxis. *Zeitschrift für Erziehungswissenschaft, 23*, 1303–1323. https://doi.org/10.1007/s11618-020-00982-6 <br>
Masnick, A. M., & Zimmerman, C. (2009). Evaluating scientific research in the context of prior belief: Hindsight bias or confirmation bias? *Journal of Psychology of Science and Technology, 2*, 29–36. https://doi.org/10.1891/1939-7054.2.1.29 <br>
Merk, S. & Rosman, T. (2019). Smart but evil? Student-Teachers’ perception of educational researchers’ epistemic trustworthiness. *AERA Open, 5*, 233285841986815. https://doi.org/10.1177/2332858419868158 <br>
Nickerson, R. S. (1998). Confirmation bias: A ubiquitous phenomenon in many guises. *Review of General Psychology, 2*, 175–220. https://doi.org/10.1037/1089-2680.2.2.175 <br>
Rosman, T., & Merk, S. (2021). Teacher’s reasons for trust and distrust in scientific evidence: Reflecting a “smart but evil” stereotype? *AERA Open, 7*, 23328584211028599. https://doi.org/10.1177/23328584211028599 <br>
Schmidt, K., Rosman, T., Cramer, C., Besa, K.-S., & Merk, S. (2022). Teachers trust educational science—Especially if it confirms their beliefs. Frontiers in Education, 7. https://doi.org/10.3389/feduc.2022.976556 <br>
Stroud, N. J. (2017). Understanding and overcoming selective exposure and judgment when communicating about science. In K. H. Jamieson, D. M. Kahan & D. A. Scheufele (Eds.) *The Oxford Handbook of the Science of Science Communication* (pp. 377–388). Oxford University Press. https://doi.org/10.1093/oxfordhb/9780190497620.013.41 <br>
Vanlommel, K., Van Gasse, R., Vanhoof, J., and Van Petegem, P. (2017). Teachers’ decision-making: Data based or intuition driven? *International Journal of Educational Research, 83*, 75–83. https://doi.org/10.1016/j.ijer.2017.02.013 <br>
Van Schaik, P., Volman, M., Admiraal, W., & Schenke, W. (2018) Barriers and conditions for teachers’ utilisation of academic knowledge. *International Journal of Educational Research, 90*, 50-63. https://doi.org/10.1016/j.ijer.2018.05.003.
:::

# <font color="#8cd000">Thank you for your attention!</font> {.cent1}
`r fontawesome::fa_i(name = "address-card")`<br>
Kirstin Schmidt <br>
Karlsruhe University of Education <br>
Bismarckstraße 10, <br>
76133 Karlsruhe <br>
kirstin.schmidt@ph-karlsruhe.de